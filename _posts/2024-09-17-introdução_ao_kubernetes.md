---
title: 01. Kubernetes, primeiros passos e primeiras informa√ß√µes
author: 01
date: 2024-04-12
categories: [kubernetes, docker]
tags: [containers, kubernetes, docker, linux]
media_subpath: /assets/media_posts/002-kubernetes01
publisehd: true
---

# 1. Antes de tudo, cont√™ineres

Da forma mais resumida poss√≠vel, Kubernetes nada mais √© do que um **"orquestrador de cont√™ineres"**. Olhando dessa maneira, fica claro que precisamos entender e dominar esses objetos para que a orquestra flua bem.

Por vezes, esse conceito pode se confundir com o [Docker](https://www.docker.com), a mais famosa ferramenta no mercado para criar e manuse√°-los, por√©m *containers* s√£o bem mais simples, eles nada mais s√£o que **uma t√©cnica de isolamente de recursos**.

Usando cont√™ineres podemos fazer com que uma aplica√ß√£o isole seus recursos computacionais (CPU, mem√≥ria, *namespaces* etc.) mesmo que divida o kernel da m√°quina com outras aplica√ß√µes. Fazendo, assim, com que tenhamos um ambiente facilmente reprodut√≠vel para rodar c√≥digos, praticamente elimenando o cl√°ssico *"mas na minha m√°quina rodou direito"*, que todo Dev um dia j√° lan√ßou.

A t√≠tulo de informa√ß√£o, podemos gerar uma √∫nica imagem, a qual ser√° a base, o espelho para que in√∫meros cont√™ineres sejam executados. Para informa√ß√µes espec√≠ficas sobre o assunto, procure um material espec√≠fico.

## 1.1. *Container Engine*

Mergulhando um pouco mais fundo no mundo da conteineriza√ß√£o das aplica√ß√µes, chegamos nos ***Container Engines***.

Eles √© que s√£o respons√°veis por gerenciar imagens e volumes dos cont√™ineres, al√©m de garantir o isolamento adequado dos recursos de cada unidade sendo executada.

O mais famoso desses √© o Docker, como citado na primeira sess√£o desse artigo, o qual foi a √∫nica op√ß√£o por muitos anos; entretanto, hoje temos novas op√ß√µes para a comunidade utilizar, algumas delas:
- [CRIO-O](https://crio-o.io);
- [Podman](https://podman.io);


## 1.2. *Container Runtime*

J√° essas *Engines* que citamos, utilizam o ***container runtime*** para executar o cont√™iner, o Docker, por exemplo, utiliza o *containerd*

No geral, temos tr√™s tipos de *runtime*:
- ***Low level:*** s√£o executados diretamente pelo kernel da m√°quina host.
    - [runc](https://github.com/opencontainers/runc);
    - [crun](https://github.com/containers/cruncon);
    - [runsc](https://github.com/google/gvisor);

- ***High level:*** s√£o executados pelas *engines* que vimos anteriormente.
    - [containerd](https://containerd.io/)

- ***Sandbox:*** s√£o *runtimes* executados pelas *engines* de forma segura. Esse tipo √© executado em UniKernel ou em algum proxy que se comunique com o kernel.
    - [gVisor](https://gvisor.dev/);
        
- ***Virtualized:*** √© executado em m√°quinas virtuais. Oferece uma performance menor, apesar de compensar com a seguran√ßa da virtualiza√ß√£o da m√°quina.
    - [Kata Containers](https://katacontainers.io/);


## 1.3. *Open Container Initiative*

Um leitor desavisado, de primeira viagem, pode se sentir muito confuso tentando identificar os padr√µes desse mundo do isolamento de recursos. 

Pode ficar se perguntando como e quem define como um cont√™iner e seus *runtimes* e *engines* devem se comportar para que outras tecnologias se acoplem com sucesso.

A resposta desse questionamento est√° na **OCI** - ***Open Container Initiative*** -, uma organiza√ß√£o sem fins lucrativos que hoje pertece ao corpo da ***The Linux Foundation***, mas foi fundada em 2015 por gigantes do ramo (Docker, CoreOS, IBM, Microsoft, RedHat e VMWare) com o objetivo de padronizar a cria√ß√£o de cont√™ires e assim garantir que estes sejam compat√≠veis com qualquer ambiente proposto.

O principal projeto da OCI, por exemplo, √© ***runc***, que j√° comentamos. Ele √© um *container runtime* de baixo n√≠vel, de c√≥digo fonte aberto escrito em Go.

&nbsp;
# 2. Finalmente Kubernetes

Diferente de outros orquestradores populares no mercado, o Kubernetes n√£o gerencia cont√™ineres diretamente, mas sim *pods*. Essas e outras estruturas que falaremos sobre agora tornam essa ferramenta √∫nica e completa.

Atrav√©s desses elementos temos a possibilidade de criar cont√™ineres e replicas, criar redes de conex√£o, padronizar IPs, liberar portas etc. Por isso e bem mais, √© imposs√≠vel falar de Kubernetes sem esmiu√ßar esses conceitos.

Por agora apenas pincelaremos os conceitos. Ao longo da leitura cara um dos elementos aqui citados ser√° debatido com mais calma

## 2.1. Pods

S√£o a menor unidade do kubernetes. Como dito anteriormente, o K8s n√£o gerencia diretamente os cont√™ineres, mas sim os pods. Podendo, estes, conter um ou mais cont√™ineres.

*Pods* nada mais s√£o que uma outra abstra√ß√£o de isolamento. Cont√™ineres que dividem o mesmo podem tamb√©m dividem o mesmo volume, endere√ßos, ciclos de CPU e mem√≥ria e outros.


## 2.2. Deployment

√â um dos principais controladores utilizados pelo Kubernetes. Ele em conjunto com os *ReplicaSet* garantem o desejado n√∫mero de r√©plicas em execu√ß√£o dos pods nos *workers*.

√â tamb√©m atrav√©s dele que gerenciamos o ciclo de vida das aplica√ß√µes, como imagens, portas, volumes, *labels* e vari√°veis de ambiente.

## 2.3. ReplicaSets

√â um objeto criado no instante de cria√ß√£o de um *Deployment*. Ele √© o bra√ßo do *Deployment* que garantir√° que o n√∫mero de pods em execu√ß√£o no n√≥ √© o desejado

## 2.4. Service

Podem ser de quatro tipos:

- *ClusterIP*
- *NodePort*
- *LoadBalancer*
- *ExternalName*

*Services* s√£o formas de criarmos, ou expormos, a comunica√ß√£oatrav√©s do cluster.

## 2.5. DaemonSet

## 2.6. StatefulSet

## 2.7. Namespaces

Tudo que opera dentro do Kuberentes opera dentro dos *namespaces*. Essas s√£o organiza√ß√µes (ou separa√ß√µes) l√≥gicas dentro da ferramente que nos permitem realizar at√© isolamento das unidades de isolamente.

Por meio deles, podemos limitar recursos, impor normas de seguran√ßa e afins.


&nbsp;
# 3. Finalmente Kubernetes (Arquitetura e Distribui√ß√µes)

Como dito antes, Kubernetes, tamb√©m apelidade de *k8s*, √© um orquestrador de cont√™ineres, ou seja, ele √© respons√°vel por todo o controle, gerenciamento, organiza√ß√£o e sa√∫de das unidades de isolamento criadas no nosso cluster computacional.

E assim como mencionamos nos cont√™ineres, o projeto Kubernetes √© soberano no mercado, mas existem alternativas a ele na orquestra√ß√£o:
- [Docker Swarm](https://docs.docker.com/engine/swarm/) (descontinuado);
- [Docker Compose](https://docs.docker.com/compose/) (apenas para desenvolvimento local);
- [RedHat OpenShift](https://www.redhat.com/pt-br/technologies/cloud-computing/openshift/container-platform);
- [HashiCorp Nomad](https://www.nomadproject.io/);
- [Apache Mesos](https://mesos.apache.org);
- [Cloud Foundry](https://https://www.cloudfoundry.org);


## 3.1. Hist√≥ria e parentes n√£o t√£o distantes

Apenas para contextualizar e marcar no tempo, o projeto que hoje chamamos de Kubernetes, surge, por volta de 2014, dentro da Google, como *Borg*, para orquestrar os cont√™ineres de servi√ßos internos da empresa.

O **Borg** deu origem ao **Omega**, o qual, por sua vez, originou o **Kubernetes**. Sendo este √∫ltimo diferenciado principalmente por ser de c√≥digo aberto.

Ainda hoje o Borg continua como o principal sistema de gerenciamento de cont√™ineres da Google devido sua capacidade de escalabilidade, robustez externa e variedade de recursos. Esses e outros motivos menores fizeram com que esse orquestrador seja a base n√£o s√≥ do Kubernetes, mas de outras ferramentas no mercado, como o **Apache Mesos** e o **Cloud Foundry**.


## 3.2. Arquitetura do Kubernetes

Apesar das diferen√ßas, os orquestradores concordam bastante quando o assunto √© a arquitetura da solu√ß√£o. Sendo assim, no Kubernetes temos:

- **Cluster Kubernetes:** um aglomerado de n√≥s/*nodes* computacionais trabalhando juntos para resolver problemas usando cont√™ineres.
  - **Nodes:** s√£o unidades computacionais distribu√≠das controladas pelo kubernetes que podem ser de dois tipos, cada um com seus componentes, responsabilidades e funcionalidades.
    
    > <u>Nota<sup>1</sup>:</u> Para garantir o bom funcionamento da aplica√ß√£o como um todo, √© recomendado trabalhar com, no m√≠nimo, 3 nodes: 1 *control-plane* e 2 *workers*;

    > <u>Nota<sup>2</sup>:</u> √â poss√≠vel criar cluster Kubernetes com menos unidades, inclusive com apenas um n√≥, onde as aplica√ß√µes rodem dentro do pr√≥prio *control-plane*, mas isso √© extramente arriscado
    
    -  **Control Plane:** √â o gerente do cluster, tem como responsabilidade garantir a sa√∫de, a disponibilidade e armazenar o estado do cluster como um todo. Por *default* n√£o rodam aplica√ß√µes;
    
    -  **Worker:** s√£o as unidades respons√°veis por executar os cont√™ineres, eles que fazem a m√°gica acontecer.

Cada um desses elementos tem seus componentes internos pr√≥prios e espec√≠ficos que realizam fun√ß√µes de monitoramento, comunica√ß√£o, execu√ß√£o etc. Mais abaixo falaremos mais sobre esses componentes.

![Arquitetura do Kubernetes](/arquitetura_cluster_kubernetes.png)


## 3.3. Componentes internos do Kubernetes

Para que esse belissimo orquestrador funcione, temos diversos componentes trabalhando por tr√°s dos panos.

Na maioria das vezes, nem iremos interagir com eles diretamente, mas a simples queda de qualquer um far√° com que o cluster come√ße a se arrastar ou at√© mesmo cair.


### 3.3.1. **API Server:** 
√â um dos principais componentes do kubernetes. √â uma API REST de JSON sobre HTTP que utiliza o famigerado *kubectl* como utilit√°rio para administrar a comunica√ß√£o entre os n√≥s.

### 3.3.2. **etcd:** 
√© um banco de dados; ou para ser mais preciso, um *datastore*; do tipo chave-valor que armazena as especifica√ß√µes, o status e as configura√ß√µes do *cluster* como um todo. Apesar de armazenar os dados do conjunto inteiro, apenas o *API Server* tem permiss√£o de gravar e ler dados do *etcd*.

Por padr√£o, ele √© executado apenas no *control-plane*, entretanto h√° casos onde podemos designar clusters apenas para rodar o *etcd*

### 3.3.3. **Scheduler:**

√â esse elemento que tem a responsabilidade de selecionar o determinado *node* que o hospedar√° determinado *pod*.

Essa sele√ß√£o √© feita baseando-se na quantidade de recursos computacionais dispon√≠veis e no estado de cada n√≥. Tudo isso para garantir que os recursos estejam bem distribu√≠dos no *cluster*.

Al√©m disso, a sele√ß√£o do n√≥ para a execu√ß√£o de um pod tamb√©m pode levar em considera√ß√£o pol√≠ticas definidas pelo usu√°rio; afinidade e localiza√ß√£o dos dados que ser√£o lidos, por exemplo.


### 3.3.4. **Controller manager:**

√â este *controller* quem garante que o *cluster* est√° rodando no √∫ltimo estado dado pelo *etcd*. Se no banco de dados temos um *deployment* pedindo 10 r√©plicas, √© o *controller manager* quem verifica constantemente se o estado atual do *cluster* corresponde ao pedido, se os estados n√£o baterem √© ele quem buscar√° atualizar o estado atual.

### 3.3.5. **Kubelet:**

Este √© o agente do Kubernetes nos *workers*. Em cada *node* deste tipo h√° um *kubelet* em execu√ß√£o encarregado de gerenciar os *pods* fornecidos pelo *controller* do *cluster*.

Por isso, ele pode iniciar, parar, pausar, resumir e manter os cont√™ineres e *pods*, sempre seguindo as instru√ß√µes fornecidas pelo controlador do *cluster*.

### 3.3.6. **Kube proxy:**

Age como *proxy* e *load balancer*, ficando respons√°vel por efetuar o roteamento de requisi√ß√µes para os devidos pods e tamb√©m por estabelecer a rede de cada n√≥.

### 3.3.7. Portas que estes elementos usam:

#### 3.3.7.1. *Control-plane*:

| Protocol | Direction | Port Range | Purpose | Used By |
|:---:|:-------:|:---------:|:------------------------|:---------------------|
| TCP |	Inbound | 6443*     | Kubernetes API server   | All                  |
| TCP |	Inbound | 2379-2380 | etcd server client API  | kube-apiserver, etcd |
| TCP |	Inbound | 10250     | Kubelet API             | Self, Control plane  |
| TCP |	Inbound | 10259     | kube-scheduler          | Self                 |
| TCP | Inbound | 10257     | kube-controller-manager |	Self                 |

* Porta customiz√°vel

#### 3.3.7.2. *Workers*:

| Protocol | Direction | Port Range | Purpose | Used By |
|:---:|:-------:|:-----------:|-------------------------|----------------------|
| TCP | Inbound | 10250       | Kubelet                 | Self, control-plane  |
| TCP | Inbound | 30000-32767 | NodePort                | Services All         |

## 3.4. Distribui√ß√µes de Kubernetes para execu√ß√£o local:

- [Kind](https://kind.sigs.k8s.io/): sigla para "*Kubernetes in Docker*". √â uma ferramenta que simula a execu√ß√£o de um cluster kubernetes usando cont√™ires Docker. √â excelente para desenvolvimento e testes, podendo criar at√© 1 *control-plane* e 3 *workers*, mas nunca deve ser usado em produ√ß√£o;

- [Minikube](https://minikube.sigs.k8s.io/docs/): √© uma ferramenta que executa um cluster kubernetes com apenas um n√≥. √â muito bom para fins did√°ticos, testes, desenvolvimento, mas nunca para produ√ß√£o;

- [MicroK8s](https://microk8s.io/): √© uma distribui√ß√£o Kubernetes desenvolvida pela **Canonical** que pode ser utilizada em ambientes de produ√ß√£o, especialmente em **IoT** (*Internet of Things*);

- [k3s](https://k3s.io/): √© tamb√©m uma distribui√ß√£o bem leve de Kubernetes, muito empregada em IoT, Raspberry PI e outros;

- [k0s](https://k0sproject.io/): desenvolvido pela Mirantis, √© uma distribui√ß√£o constru√≠da para conter todos os recursos necess√°rios para funcionar com um √∫nico bin√°rio. Pode at√© ser utilizado em ambientes de produ√ß√£o em casos espec√≠ficos.

&nbsp;
# 4. Finalmente instalando o Kubernetes (localmente)


## 4.1. Kubectl

O primeiro passo para interagir com um cluster Kubernetes √© dispor do `kubectl`. Para instal√°-lo, basta executar no shell da sua m√°quina:

```bash
$ curl -LO "https://dl.k8s.io/release/$(\
    curl -L -s https://dl.k8s.io/release/stable.txt\
    )/bin/linux/amd64/kubectl"
$ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

E verificar a instala√ß√£o com:
```bash
$ kubectl version --client
```

Esse utilit√°rio funciona de forma bem simples e intuitiva: `kubectl` + \<verbo> + \<recurso>. Ent√£o, por exemplo, 
- se queremos criar um pod: `kubectl create pod`; 
- se queremos pegar as informa√ß√µes de um *deployment*: `kubectl get deployment`; 
- se queremos deletar um *service*: `kubectl delete service`. 

E por a√≠ vai.

## 4.2. O bendito cluster

H√° diversas formas diferentes de se criar um cluster kubernetes local. Na sess√£o #1 desse artigo listamos v√°rias, a titulo de exemplo. 

E aqui repetiremos algumas para efetivamente instalar e usar alguns comandos. 

### 4.2.1. Minikube

Esta distribui√ß√£o funciona na base da virtualiza√ß√£o, ou seja, ela cria uma m√°quina virtual e dentro dela o cluster Kuberentes. Para isso, √© necess√°rio garantir que a sua m√°quina suporte essa opera√ß√£o; no GNU/Linux podemos fazer a checagem com o seguinte comando:

```bash
$ grep -E --color 'vmx|svm' /proc/cpuinfo
```

Qualquer sa√≠da diferente de vazio √© uma resposta positiva.

Ent√£o instalamos o Minikube, em si:

```bash
$ curl -Lo \
    minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

$ chmod +x ./minikube

$ sudo mv ./minikube /usr/local/bin/minikube
```

E verificamos a instala√ß√£o:
```bash
$ minikube version
```

Feito isso, podemos iniciar o Cluster Local:
```bash
minikube start
```
Ou ainda, `minikube start --nodes <NUM_NODES> -p multinode-cluste`, se quisermos um cluster com mais de um n√≥. Se tudo correr bem, teremos uma resposta do tipo:
```bash
üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Using the qemu2 (experimental) driver based on user configuration
üëç  Starting control plane node minikube in cluster minikube
üî•  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: default-storageclass, storage-provisioner
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

A partir da√≠, podemos descobrir as funcionalidades do `kubectl` como bem quisermos:
```bash
$ kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   20s   v1.25.3
```

O Minikube tem a regalia de nos fornecer diversos comandos que facilitaram o ambiente de testes e desenvolvimento:
- `minikube dashboard`; para habilitar o dashboard web;
- `minikube delete`: para deletar o cluster;
- `minikube delete`: para deletar o cluster e todos os arquivos referentes a ele;
- `minikube ip`: para fornecer o endere√ßo da m√°quina virtual;
- `minikube logs`: para verificar os logs do cluster;
- `minikube ssh`: para acessar a m√°quina virtual criada;
- `minikube status`: para pegar informa√ß√µes sobre o cluster;


### 4.2.2. Kind

Essa √© a minha distribui√ß√£o favorita para testes por dois motivos. 

Primeiramente, por pura predile√ß√£o, foi a primeira que botei a m√£o, ent√£o acaba que √© a que mais tenho intimidade. 

Segundamente, ela me permite passar um arquivo `.yaml`
```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```
descrevendo como quero esse cluster, o que permite mais organiza√ß√£o e clareza no desenvolvimento.

Para instalar √© bem simples:
```bash
$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64

$ chmod +x ./kind

$ sudo mv ./kind /usr/local/bin/kind
```

Feito isso, podemos criar o cluster:
```bash
kind create cluster
```
Ou
```bash
kind create cluster --name kind-multinodes --config ./kind-3nodes.yaml
```
E teremos uma resposta similar a:
```bash
Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
```

&nbsp;
# 5. O Kubernetes no mundo real

Hoje o kubernetes √© t√£o consolidado no ecossistema DevOps que algumas pessoas costumam dizer que ele √© "o novo linux", isso porque h√° dezenas de diferentes distribui√ß√µes operando em diferentes mercados e para diferentes fun√ß√µes e um ecossistema gigantesco de ferramentas que operam nele e com ele. 

Grandes empresas, como Rancher e RedHat, por exemplo, criando seu pr√≥prio *superset* de funcionalidades para oferecer aos cliente um *starter* pr√©-fabricado.

H√° ainda, Cloud Providers, como AWS, GCP e Azure criando suas "m√°quinas kubernetes" desde o provisionamento.

Voc√™ pode ver mais sobre as ferramentas que operam na Cloud e interagem muito bem entre si na [landscape da CNCF](https://landscape.cncf.io/).


&nbsp;
# 6. Brincando com Kuberentes

## 6.1. Verificando nos namespace:

Nativamente alguns pods j√° vem em execu√ß√£o. Podemos visualiz√°-los de duas formas:

1. Listando todos os pods com a tag `-A`: `kubectl get pods -A`
2. Verificando direto no *namespace* `kube-system`: `kubernetes get pods -n kube-system`

De ambas as formas temos resultados similares.

## 6.2. Criando o primeiro pod

Podemos executar um pod com um cont√™iner nginx com o seguinte comando:
```bash
$ kubectl run nginx --image nginx
pod/nginx created
```

E verificar seu estado:
```bash
$ kubectl get pods -A
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
```

Delet√°-lo:
```bash
$ kubectl delete pod nginx
pod "nginx" deleted
```

## 6.3. Templates com DryRun

Como √© de se esperar, n√£o √© uma boa pr√°tica criar alterar objetos no geral atrav√©s da linha de comando. A melhor op√ß√£o sempre ser√° documentar as vers√µes atrav√©s de arquivos manifestos descrevendo detalhe a detalhe.

E a melhor e mais r√°pida op√ß√£o para criar esses arquivos provalmente √© o `dry-run`. 

Utilizando o exemplo do nginx,far√≠amos da seguinte maneira:
```bash
$ kubectl run nginx --image nginx --port 80 --dry-run=client -o yaml > pod-template.yaml
```

E a sa√≠da, o arquivo `pod-template.yaml`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

Pronto para uso, ou para modifica√ß√µes. Se fizermos, `kubectl apply -f pod-template.yaml`, por exemplo, teremos um pod rodando um cont√™iner nginx.

A partir desse pod, poder√≠amos executar tamb√©m,
```bash
$ kubectl expose pod nginx
```

Para exp√¥-lo a rede. Da√≠ ter√≠amos os services `kubernetes`, padr√£o, e `nginx`:
```bash
$ kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8d
nginx        ClusterIP   10.105.41.192   <none>        80/TCP    2m30s
```

## 6.4. Visualizando recursos

H√° outras op√ß√µes bem interessante dentro do leque de fun√ß√µes que o `kubectl` nos permite:
- `kubectl get all -A`: para listar todos os objetos em todos os *namespaces*;
- `kubectl get pod, service`: para listar *pods* e *services* no namespace *default*;
- `kubectl delete -f pod-template.yaml `: para deletar os objetos relacionados √†quele arquivo de configura√ß√µes;
- `kubectl delete service nginx`: para deletar o service de nome nginx no *namespace* *default*;


# 7. Refer√™ncias

Minha maior refer√™ncia para esse artigo √© o [primeiro dia do Descomplicando Kuberentes da Linux Tips](https://github.com/badtuxx/DescomplicandoKubernetes/blob/main/pt/day-1/README.md#arquitetura-do-k8s), mas al√©m disso posso citar umas boas aulas que tive a oportunidade de ganhar com o Ruan, cabe√ßa da JackExperts.

Fora isso, a internet √© um mund√£o e esse artigo estar√° sempre em expans√£o.
